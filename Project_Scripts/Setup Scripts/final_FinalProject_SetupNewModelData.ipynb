{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "automotive-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "swedish-retreat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-opposition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "intermediate-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(sent_list,split_percent):\n",
    "    split_val = int(len(sent_list)*split_percent)\n",
    "    train_list = sent_list[:split_val]\n",
    "    val_list = sent_list[split_val:]\n",
    "    \n",
    "    return train_list, val_list\n",
    "\n",
    "def remove_sgm(sent):\n",
    "    return re.sub('<seg id=\"[0-9]+\">|<\\/seg>|\\\\n', '', sent)\n",
    "\n",
    "def output_list_2_txt(my_list,output_path):\n",
    "    with open(output_path,\"w\") as f:\n",
    "        for sent in my_list:\n",
    "            f.write(\"%s\\n\" % sent)\n",
    "        f.flush()\n",
    "        \n",
    "def filter_sentences(en_list,de_list,min_length, max_length):\n",
    "    new_en_list = []\n",
    "    new_de_list = []\n",
    "    sent_dropped = 0\n",
    "\n",
    "    for i in range(len(en_list)):\n",
    "        if len(en_list[i]) >= min_length and len(en_list[i]) <= max_length:\n",
    "            if len(de_list[i]) >= min_length and len(de_list[i]) <= max_length:\n",
    "                new_en_list.append(en_list[i])\n",
    "                new_de_list.append(de_list[i])\n",
    "            else:\n",
    "                sent_dropped +=1     \n",
    "        else:\n",
    "            sent_dropped +=1\n",
    "\n",
    "    if len(new_en_list) == len(new_de_list):\n",
    "        print(\"Length of both lists match {}\".format(len(new_de_list)))\n",
    "    else:\n",
    "        print(\"Somethings gone terribly wrong!!\")\n",
    "    return new_en_list, new_de_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "outside-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosestokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cloudy-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MosesTokenizer('en')\n",
    "detokenizer = MosesDetokenizer('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cordless-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<seg id=\"0\">But it was. Specifically, what was broken was the story assigning value to mortgage-backed securities and other derivatives based on unrepayable loans.</seg>\n",
      "\n",
      "<seg id=\"99999\">I was not even out rightly afraid.</seg>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/home/jupyter/Getting_New_Data/paracrawl_data/test/paracrawl-ende-src.en.sgm\", \"r\")\n",
    "en_list = f.readlines()\n",
    "#Remove non applicable lines\n",
    "en_list = en_list[3:-3]\n",
    "print(en_list[0])\n",
    "print(en_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "polished-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was jedoch konkret gebrochen wurde, war die Geschichte, die hypothekenbesicherten Wertpapieren und anderen Derivaten auf der Grundlage nicht r端ckzahlbarer Kredite Wert zuweist.\n",
      "\n",
      "Ich hatte nicht einmal zu Recht Angst.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/home/jupyter/Getting_New_Data/paracrawl_data/paracrawl-ende-src.hyp.detok.de\", \"r\")\n",
    "de_list = f.readlines()\n",
    "print(de_list[0])\n",
    "print(de_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "residential-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of en list:100000\n",
      "But it was. Specifically, what was broken was the story assigning value to mortgage-backed securities and other derivatives based on unrepayable loans.\n",
      "Was jedoch konkret gebrochen wurde, war die Geschichte, die hypothekenbesicherten Wertpapieren und anderen Derivaten auf der Grundlage nicht r端ckzahlbarer Kredite Wert zuweist.\n",
      "I was not even out rightly afraid.\n",
      "Ich hatte nicht einmal zu Recht Angst.\n"
     ]
    }
   ],
   "source": [
    "#Remove SGM formatting\n",
    "en_list = list(map(remove_sgm,en_list))\n",
    "de_list = list(map(remove_sgm,de_list))\n",
    "print(\"Length of en list:{}\".format(len(en_list)))\n",
    "print(en_list[0])\n",
    "print(de_list[0])\n",
    "print(en_list[-1])\n",
    "print(de_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "valuable-unemployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokenised en list:100000\n",
      "['But', 'it', 'was', '.', 'Specifically', ',', 'what', 'was', 'broken', 'was', 'the', 'story', 'assigning', 'value', 'to', 'mortgage', '@-@', 'backed', 'securities', 'and', 'other', 'derivatives', 'based', 'on', 'unrepayable', 'loans', '.']\n",
      "['Was', 'jedoch', 'konkret', 'gebrochen', 'wurde', ',', 'war', 'die', 'Geschichte', ',', 'die', 'hypothekenbesicherten', 'Wertpapieren', 'und', 'anderen', 'Derivaten', 'auf', 'der', 'Grundlage', 'nicht', 'r端ckzahlbarer', 'Kredite', 'Wert', 'zuweist', '.']\n",
      "['I', 'was', 'not', 'even', 'out', 'rightly', 'afraid', '.']\n",
      "['Ich', 'hatte', 'nicht', 'einmal', 'zu', 'Recht', 'Angst', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenise Sentences\n",
    "en_list_token = list(map(tokenizer,en_list))\n",
    "de_list_token = list(map(tokenizer,de_list))\n",
    "print(\"Length of tokenised en list:{}\".format(len(en_list_token)))\n",
    "print(en_list_token[0])\n",
    "print(de_list_token[0])\n",
    "print(en_list_token[-1])\n",
    "print(de_list_token[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "attractive-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of both lists match 99673\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "min_length = 3\n",
    "sent_dropped = 0\n",
    "new_en_list, new_de_list = filter_sentences(en_list_token,de_list_token,min_length, max_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dense-flight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But it was. Specifically, what was broken was the story assigning value to mortgage-backed securities and other derivatives based on unrepayable loans.\n",
      "I was not even out rightly afraid.\n",
      "Was jedoch konkret gebrochen wurde, war die Geschichte, die hypothekenbesicherten Wertpapieren und anderen Derivaten auf der Grundlage nicht r端ckzahlbarer Kredite Wert zuweist.\n",
      "Ich hatte nicht einmal zu Recht Angst.\n"
     ]
    }
   ],
   "source": [
    "#De-Tokenise EN Sentences\n",
    "en_list = list(map(detokenizer,new_en_list))\n",
    "de_list = list(map(detokenizer,new_de_list))\n",
    "#De-Tokenise DE Sentences\n",
    "de_list = list(map(detokenizer,new_de_list))\n",
    "print(en_list[0])\n",
    "print(de_list[0])\n",
    "print(en_list[-1])\n",
    "print(de_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "appropriate-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_list = en_list_token\n",
    "de_list = de_list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "further-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest value 250\n",
      "Smallest value 0\n"
     ]
    }
   ],
   "source": [
    "biggest =  int(0)\n",
    "smallest = int(0)\n",
    "for i in range(len(en_list)):\n",
    "    if len(en_list[i]) >= biggest:\n",
    "        biggest = len(en_list[i])\n",
    "        \n",
    "    if len(de_list[i]) >= biggest:\n",
    "        biggest = len(de_list[i])\n",
    "    \n",
    "    if len(en_list[i]) <= smallest:\n",
    "        smallest = len(en_list[i])\n",
    "    \n",
    "    if len(de_list[i]) <= smallest:\n",
    "        smallest = len(de_list[i])\n",
    "\n",
    "print(\"Biggest value {}\".format(biggest))\n",
    "print(\"Smallest value {}\".format(smallest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "worldwide-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training dataset :98177, length of validation dataset:1496\n"
     ]
    }
   ],
   "source": [
    "split_p = 0.985\n",
    "de_train, de_val = train_val_split(de_list,split_p)\n",
    "en_train, en_val = train_val_split(en_list,split_p)\n",
    "print('length of training dataset :{}, length of validation dataset:{}'.format(len(de_train),len(de_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "supreme-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/home/jupyter/Getting_New_Data/NewBasicModel/dataset'\n",
    "output_de_train = output_dir+'/tgt-train.txt'\n",
    "output_de_val = output_dir+'/tgt-val.txt'\n",
    "output_en_train = output_dir+'/src-train.txt'\n",
    "output_en_val = output_dir+'/src-val.txt'\n",
    "\n",
    "output_list_2_txt(de_list,output_de_train)\n",
    "output_list_2_txt(de_list,output_de_val)\n",
    "output_list_2_txt(en_list,output_en_train)\n",
    "output_list_2_txt(en_list,output_en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-sleeping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-barrel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-rebate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-superintendent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-consortium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-despite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-commons",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-amount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-missile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
